{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a038d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from transformers import BertTokenizer, BertConfig, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce9eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "max_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5c9fc87-3d9a-4bf3-b9e9-ea8e89e6a6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_json = json.load(open('data/train_set.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcc1b196-f1b7-4974-ae00-6edcbfea0dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(train_json.values())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6db6c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for loading PubMedQA data from JSON files\n",
    "class PubMedQADataset(Dataset):\n",
    "    def __init__(self, json_file, tokenizer, max_length=512, labeled=True):\n",
    "        self.data = list(json.load(open(json_file)).values())\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.labeled = labeled\n",
    "\n",
    "        self.label_map = {\n",
    "            \"yes\": 0,\n",
    "            \"no\": 1,\n",
    "            \"maybe\": 2,\n",
    "            \"no_label\":3\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.data[idx]['QUESTION']\n",
    "        context = self.data[idx]['CONTEXTS']\n",
    "        inputs = self.tokenizer(question, context, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "        if self.labeled:\n",
    "            label = self.data[idx]['final_decision']\n",
    "            item['labels'] = torch.tensor(self.label_map[label])\n",
    "        else:\n",
    "            item['labels'] = torch.tensor(self.label_map[\"no_label\"])\n",
    "\n",
    "        return item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17e925d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load datasets\n",
    "tokenizer = BertTokenizer.from_pretrained('nlpie/tiny-biobert')\n",
    "labeled_dataset = PubMedQADataset('data/train_set.json', tokenizer,max_length=max_len, labeled=True)\n",
    "artificial_dataset = PubMedQADataset('data/ori_pqaa.json',tokenizer,max_length=max_len,labeled=True)\n",
    "unlabeled_dataset = PubMedQADataset('data/ori_pqau.json', tokenizer, max_length=max_len,labeled=False)\n",
    "\n",
    "# Create DataLoaders\n",
    "labeled_loader = DataLoader(labeled_dataset, batch_size=8, shuffle=True)\n",
    "artificial_loader =  DataLoader(artificial_dataset, batch_size=8, shuffle=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "391ba46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at nlpie/tiny-biobert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class TransformerGenerator(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased'):\n",
    "        super(TransformerGenerator, self).__init__()\n",
    "        self.config = BertConfig.from_pretrained(model_name)\n",
    "        self.bert = BertModel.from_pretrained(model_name, config=self.config)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, self.config.vocab_size)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        logits = self.fc(hidden_states)\n",
    "        return logits\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "generator = TransformerGenerator('nlpie/tiny-biobert').to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e0c548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpie/tiny-biobert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "discriminator = BertForSequenceClassification.from_pretrained('nlpie/tiny-biobert', num_labels=4).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b51f47a-621e-4b98-9990-488dcae54f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenate labeled and unlabeled datasets for joint training\n",
    "combined_dataset = ConcatDataset([labeled_dataset, artificial_dataset,unlabeled_dataset])\n",
    "combined_loader = DataLoader(combined_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6680ea5a-dc98-4198-8ff5-6cd0265a287f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8623b476934abf9b03c7433719d42f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/273018 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_class_counts(dataset):\n",
    "    class_counts = Counter()\n",
    "    for data in tqdm(dataset):\n",
    "        label = data['labels'].item()\n",
    "        class_counts[label] += 1\n",
    "    return class_counts\n",
    "\n",
    "# Compute class counts and weights\n",
    "class_counts = compute_class_counts(combined_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cd0a3b2-a0d3-403f-b34e-9e14c1fd4ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count_list = [class_counts[i] for i in range(len(class_counts))]\n",
    "class_weights = [max(class_count_list) / count for count in class_count_list]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to('cuda')\n",
    "\n",
    "# Create the criterion with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcabb297-4047-4968-8277-b5ed6342b426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0: 196420, 3: 61249, 1: 15294, 2: 55}), [196420, 15294, 55, 61249])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts,class_count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4cd1dba-0dbf-4139-82da-4ac4fed6410d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 1.2843e+01, 3.5713e+03, 3.2069e+00], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76c76dd2-84e4-41ce-829b-9b7f9e5ccd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "# Optimizers\n",
    "g_optimizer = AdamW(generator.parameters(), lr=1e-4)\n",
    "d_optimizer = AdamW(discriminator.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f85c178-d862-4ce0-bcde-f18f05df496a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26ee2ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a423909d878141c199fb4e3f91accfe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/1:   0%|          | 0/34128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/1 | Discriminator Loss: 0.38557541370391846 | Generator Loss: 0.0003756763762794435\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Loss function\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(g_start_epoch,g_start_epoch+num_epochs):\n",
    "    discriminator.train()\n",
    "    generator.train()\n",
    "    progress_bar = tqdm(combined_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        real_data = batch['input_ids'].to('cuda')\n",
    "        attention_mask = batch['attention_mask'].to('cuda')\n",
    "        \n",
    "        # If the batch contains labeled data, use labels; otherwise, generate fake labels\n",
    "        if 'labels' in batch:\n",
    "            labels = batch['labels'].to('cuda')\n",
    "        else:\n",
    "            labels = torch.zeros(real_data.size(0), dtype=torch.long).to('cuda')\n",
    "\n",
    "        # Discriminator forward pass\n",
    "        d_optimizer.zero_grad()\n",
    "        outputs = discriminator(input_ids=real_data, attention_mask=attention_mask).logits\n",
    "        d_loss_real = criterion(outputs, labels)\n",
    "\n",
    "        # Generator forward pass\n",
    "        noise = torch.randint(0, generator.config.vocab_size, (real_data.size(0), real_data.size(1))).to('cuda')\n",
    "        fake_data_logits = generator(input_ids=noise, attention_mask=(noise != tokenizer.pad_token_id).to('cuda'))\n",
    "        fake_data = torch.argmax(fake_data_logits, dim=-1)\n",
    "        fake_labels = torch.zeros(real_data.size(0), dtype=torch.long).to('cuda')  # Fake labels are zeros\n",
    "\n",
    "        d_loss_fake = criterion(discriminator(input_ids=fake_data, attention_mask=(fake_data != tokenizer.pad_token_id).to('cuda')).logits, fake_labels)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Generator training\n",
    "        g_optimizer.zero_grad()\n",
    "        noise = torch.randint(0, generator.config.vocab_size, (real_data.size(0), real_data.size(1))).to('cuda')\n",
    "        fake_data_logits = generator(input_ids=noise, attention_mask=(noise != tokenizer.pad_token_id).to('cuda'))\n",
    "        fake_data = torch.argmax(fake_data_logits, dim=-1)\n",
    "        g_loss = criterion(discriminator(input_ids=fake_data, attention_mask=(fake_data != tokenizer.pad_token_id).to('cuda')).logits, fake_labels)\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step() \n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} | Discriminator Loss: {d_loss.item()} | Generator Loss: {g_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74d5980a-58a8-43eb-9806-ece547719307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[:,:-1].shape\n",
    "\n",
    "0.82, 0.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "995866c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 40.60%\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "test_dataset = PubMedQADataset('data/test_set.json', tokenizer, labeled=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Evaluation\n",
    "discriminator.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = batch['input_ids'].to('cuda')\n",
    "        labels = batch['labels'].to('cuda')\n",
    "        outputs = discriminator(input_ids=inputs, attention_mask=batch['attention_mask'].to('cuda')).logits\n",
    "        _, predicted = torch.max(outputs[:,:-1], 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0131c75-ced1-4d73-a17d-da60b8b0620c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labelled Train Accuracy: 39.80%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Evaluation\n",
    "discriminator.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in labeled_loader:\n",
    "        inputs = batch['input_ids'].to('cuda')\n",
    "        labels = batch['labels'].to('cuda')\n",
    "        outputs = discriminator(input_ids=inputs, attention_mask=batch['attention_mask'].to('cuda')).logits\n",
    "        _, predicted = torch.max(outputs[:,:-1], 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Labelled Train Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5799f4f-7e4e-4b9d-8968-6b1530a14c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35e23d139824622bb9a6f59375974b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Train Accuracy: 92.20%\n"
     ]
    }
   ],
   "source": [
    "discriminator.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "artificial_loader =  DataLoader(artificial_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(artificial_loader):\n",
    "        inputs = batch['input_ids'].to('cuda')\n",
    "        labels = batch['labels'].to('cuda')\n",
    "        outputs = discriminator(input_ids=inputs, attention_mask=batch['attention_mask'].to('cuda')).logits\n",
    "        _, predicted = torch.max(outputs[:,:-1], 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Artificial Train Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2e129a-668f-4a62-a2af-74773d2e224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                                    Epoch\n",
    "Artificial Train Accuracy: 59.19%,87.26%,89.23%,92.20\n",
    "Labelled Train Accuracy: 31.80%,37%,41.60%,39.80\n",
    "Test Accuracy: 30.60%,35.., 44.40%,40.60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa97ca89-8552-422e-a3f0-3d5aa9676df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"weights\"\n",
    "generator_model_path = os.path.join(save_dir, \"generator_model.pth\")\n",
    "discriminator_model_path = os.path.join(save_dir, \"discriminator_model.pth\")\n",
    "g_optimizer_path = os.path.join(save_dir, \"g_optimizer.pth\")\n",
    "d_optimizer_path = os.path.join(save_dir, \"d_optimizer.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "146eacc3-a95c-42db-a92b-dedfeddfa65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save({\n",
    "        'epoch': 2,\n",
    "        'model_state_dict': generator.state_dict(),\n",
    "        'optimizer_state_dict': g_optimizer.state_dict(),\n",
    "        'loss': g_loss.item()\n",
    "    }, generator_model_path)\n",
    "\n",
    "torch.save({\n",
    "        'epoch': 2,\n",
    "        'model_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_state_dict': d_optimizer.state_dict(),\n",
    "        'loss': d_loss.item()\n",
    "    }, discriminator_model_path)\n",
    "\n",
    "torch.save(g_optimizer.state_dict(), g_optimizer_path)\n",
    "torch.save(d_optimizer.state_dict(), d_optimizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "515088a5-5b8f-464d-8e76-a3c4d11107fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model and optimizer state dicts\n",
    "checkpoint = torch.load(generator_model_path)\n",
    "generator.load_state_dict(checkpoint['model_state_dict'])\n",
    "g_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "g_start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "checkpoint = torch.load(discriminator_model_path)\n",
    "discriminator.load_state_dict(checkpoint['model_state_dict'])\n",
    "d_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "d_start_epoch = checkpoint['epoch'] + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045df1b6-ba0f-4185-b77f-11b2d245cb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba216db-dc38-4dc9-88eb-b14397a4fbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8dc0f8-4775-4a68-9629-42cc352ebb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test Accuracy: 30.60%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
